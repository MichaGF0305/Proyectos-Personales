{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from ultralytics import YOLO\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga del Modelo y del Video a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo preentrenado\n",
    "model = YOLO('yolov8m.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui se abre el video \n",
    "cap = cv.VideoCapture('V_Mall.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuraciones inciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se configura la resolución del video capturado\n",
    "cap.set(cv.CAP_PROP_FRAME_WIDTH, 1024)  # Aquí establecemos el ancho \n",
    "cap.set(cv.CAP_PROP_FRAME_HEIGHT, 576)  # Aquí establecemos el alto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este es el umbral de confianza para las detecciones\n",
    "confidence_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables para poder rastrear posiciones y valores persistentes\n",
    "previous_positions = {}\n",
    "frame_count = 0\n",
    "person_count = 0\n",
    "density = 0\n",
    "moving_ratio = 0\n",
    "occupied_area_ratio = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del VideoWriter\n",
    "frame_width = int(cap.get(cv.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv.CAP_PROP_FPS))\n",
    "\n",
    "output_filename = 'video_resultado.avi'\n",
    "fourcc = cv.VideoWriter_fourcc(*'XVID')  # Codec para AVI\n",
    "out = cv.VideoWriter(output_filename, fourcc, fps, (frame_width, frame_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento del Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 11 persons, 1 handbag, 483.5ms\n",
      "Speed: 16.8ms preprocess, 483.5ms inference, 15.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 handbags, 425.8ms\n",
      "Speed: 4.6ms preprocess, 425.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 handbags, 350.9ms\n",
      "Speed: 3.0ms preprocess, 350.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 5 handbags, 320.2ms\n",
      "Speed: 2.0ms preprocess, 320.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 handbags, 301.0ms\n",
      "Speed: 8.0ms preprocess, 301.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 handbags, 389.3ms\n",
      "Speed: 4.0ms preprocess, 389.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 4 handbags, 302.1ms\n",
      "Speed: 4.0ms preprocess, 302.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 4 handbags, 300.1ms\n",
      "Speed: 4.0ms preprocess, 300.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 handbags, 333.4ms\n",
      "Speed: 3.0ms preprocess, 333.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 handbags, 307.6ms\n",
      "Speed: 3.0ms preprocess, 307.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 handbags, 373.6ms\n",
      "Speed: 4.0ms preprocess, 373.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 handbags, 585.8ms\n",
      "Speed: 3.2ms preprocess, 585.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 handbag, 727.5ms\n",
      "Speed: 5.6ms preprocess, 727.5ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 handbags, 688.5ms\n",
      "Speed: 9.1ms preprocess, 688.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 handbags, 654.2ms\n",
      "Speed: 4.5ms preprocess, 654.2ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 handbags, 634.2ms\n",
      "Speed: 5.7ms preprocess, 634.2ms inference, 6.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 handbags, 616.6ms\n",
      "Speed: 10.1ms preprocess, 616.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 handbags, 590.9ms\n",
      "Speed: 9.2ms preprocess, 590.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 handbags, 632.1ms\n",
      "Speed: 5.9ms preprocess, 632.1ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 4 handbags, 603.7ms\n",
      "Speed: 4.8ms preprocess, 603.7ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 4 handbags, 616.0ms\n",
      "Speed: 10.3ms preprocess, 616.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 handbags, 632.3ms\n",
      "Speed: 3.9ms preprocess, 632.3ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 handbags, 607.2ms\n",
      "Speed: 10.6ms preprocess, 607.2ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 4 handbags, 635.6ms\n",
      "Speed: 8.7ms preprocess, 635.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 4 handbags, 619.8ms\n",
      "Speed: 10.3ms preprocess, 619.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 handbags, 565.4ms\n",
      "Speed: 7.4ms preprocess, 565.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 3 handbags, 605.7ms\n",
      "Speed: 7.5ms preprocess, 605.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# Procesar video\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Fin del video.\")\n",
    "        break\n",
    "\n",
    "    # Realizar detección de objetos con YOLOv8\n",
    "    results = model(frame)\n",
    "\n",
    "    # Obtener las detecciones en formato xyxy\n",
    "    detections = results[0].boxes\n",
    "\n",
    "    # Variables para conteo y cálculo de densidad\n",
    "    current_person_count = 0\n",
    "    moving_count = 0\n",
    "    static_count = 0\n",
    "    total_box_area = 0\n",
    "    current_positions = {}\n",
    "\n",
    "    for det in detections:\n",
    "        # Los resultados se almacenan como [x1, y1, x2, y2, conf, class_id]\n",
    "        x1, y1, x2, y2 = det.xyxy[0]\n",
    "        conf = det.conf[0]\n",
    "        class_id = det.cls[0]\n",
    "\n",
    "        if conf > confidence_threshold and class_id == 0:  # Clase 'person'\n",
    "            # Incrementar el conteo de personas\n",
    "            current_person_count += 1\n",
    "\n",
    "            # Dibujar las cajas de las personas detectadas\n",
    "            cv.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 4)\n",
    "            cv.putText(frame, f'Person: {conf:.2f}', (int(x1), int(y1) - 10),\n",
    "                       cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            # Guardar la posición actual de la persona\n",
    "            current_positions[current_person_count] = ((x1 + x2) / 2, (y1 + y2) / 2)\n",
    "\n",
    "            # Calcular el área ocupada por la caja\n",
    "            total_box_area += (x2 - x1) * (y2 - y1)\n",
    "\n",
    "    # Actualizar valores solo si hubo detecciones\n",
    "    if current_person_count > 0:\n",
    "        person_count = current_person_count\n",
    "\n",
    "        # Calcular densidad de personas\n",
    "        frame_area = frame.shape[0] * frame.shape[1]\n",
    "        density = person_count / frame_area if frame_area > 0 else 0\n",
    "\n",
    "        # Determinar si las personas están en movimiento o estáticas\n",
    "        for person_id, current_pos in current_positions.items():\n",
    "            if person_id in previous_positions:\n",
    "                prev_pos = previous_positions[person_id]\n",
    "                distance = ((current_pos[0] - prev_pos[0]) ** 2 +\n",
    "                            (current_pos[1] - prev_pos[1]) ** 2) ** 0.5\n",
    "                if distance > 10:  # Umbral para considerar \"en movimiento\"\n",
    "                    moving_count += 1\n",
    "                else:\n",
    "                    static_count += 1\n",
    "            else:\n",
    "                static_count += 1\n",
    "\n",
    "        # Calcular proporción de movimiento\n",
    "        total_detected = moving_count + static_count\n",
    "        moving_ratio = moving_count / total_detected if total_detected > 0 else 0\n",
    "\n",
    "        # Calcular porcentaje de área ocupada por las cajas\n",
    "        occupied_area_ratio = total_box_area / frame_area if frame_area > 0 else 0\n",
    "\n",
    "    # Actualizar posiciones para el siguiente frame\n",
    "    previous_positions = current_positions\n",
    "\n",
    "    # Cambiar color del texto basado en el conteo de personas\n",
    "    text_color = (0, 255, 0) if person_count < 10 else (0, 0, 255)\n",
    "\n",
    "    # Mostrar indicadores en el frame\n",
    "    cv.putText(frame, f'Personas: {person_count}', (10, 40),\n",
    "               cv.FONT_HERSHEY_SIMPLEX, 1, text_color, 2)\n",
    "    cv.putText(frame, f'Densidad: {density:.6f}', (10, 80),\n",
    "               cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    cv.putText(frame, f'Proporcion de Movimiento: {moving_ratio:.2f}', (10, 120),\n",
    "               cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    cv.putText(frame, f'Area Ocupada: {occupied_area_ratio:.2%}', (10, 160),\n",
    "               cv.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
    "\n",
    "    # Redimensionar el frame antes de mostrarlo\n",
    "    frame_resized = cv.resize(frame, (1024, 576))\n",
    "\n",
    "    # Mostrar el frame con las detecciones\n",
    "    cv.imshow('Detección en Vivo', frame_resized)\n",
    "\n",
    "    # Mostrar el frame procesado\n",
    "    frame_resized = cv.resize(frame, (1024, 576))\n",
    "    cv.imshow('Detección en Vivo', frame_resized)\n",
    "\n",
    "    # Escribir el frame procesado en el archivo de video\n",
    "    out.write(frame_resized)\n",
    "\n",
    "    # Salir si se presiona la tecla 'q'\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "# Liberar recursos y cerrar la ventana\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
